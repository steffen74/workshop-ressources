{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Example call of the pipeline function to perform named entity recognition (NER) on a text"
      ],
      "metadata": {
        "id": "yeswkNKAPnKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ONb6mUqcOmaD",
        "outputId": "61ad80bd-b249-413e-82f2-9e4123e129dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9967545866966248}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "sequence = \"Ive been waiting for an introduction to understand Hugging Face my whole life!.\"\n",
        "print(nlp(sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example for zero shot classification\n"
      ],
      "metadata": {
        "id": "4vAXI9HIPyTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i5fOIIdFOmaF",
        "outputId": "f6038931-4902-45b5-a96e-dab79d40a761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'This is a presentation about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8598707318305969, 0.09646529704332352, 0.04366389289498329]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a presentation about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example for text generation with an explicit model"
      ],
      "metadata": {
        "id": "a459VZRBP7aX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V_TfFkzbOmaG",
        "outputId": "7a524369-a1a8-4ab9-ea7c-94c5997359c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this session, we will teach you how to use a new virtual reality system, our VR solution for the Rift, what you want to learn first'},\n",
              " {'generated_text': 'In this session, we will teach you how to write code and learn how to write and test your code. We will also give you a fun and'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this session, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example for text generation without using the pipeline function"
      ],
      "metadata": {
        "id": "CfB9OViMQJW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ee3NDJJHOmaH",
        "outputId": "aa10e0bd-ccbe-4bc6-c7d7-5d1f6d7da49d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing tokenizer and model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialization complete.\n",
            "\n",
            "Padding token set to EOS token.\n",
            "\n",
            "Processing the following prompts:\n",
            "- I've been waiting for a HuggingFace course my whole life,\n",
            "- I hate this so much,\n",
            "\n",
            "Tokenizing inputs...\n",
            "Tokens (input IDs): tensor([[   40,  1053,   587,  4953,   329,   257, 12905,  2667, 32388,  1781,\n",
            "           616,  2187,  1204,    11],\n",
            "        [   40,  5465,   428,   523,   881,    11, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256]])\n",
            "Attention masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Tokenization complete.\n",
            "\n",
            "Generating text...\n",
            "Tokens (output IDs): tensor([[   40,  1053,   587,  4953,   329,   257, 12905,  2667, 32388,  1781,\n",
            "           616,  2187,  1204,    11,   290,   314,  1101,   523,  9675,   314,\n",
            "           750,    13,   314,  1101,   523,  3772,   284,   307,  1498,   284,\n",
            "          2648,   616,  1998,   351,   345,    13,   198,   198,    40,  1101,\n",
            "           523,  3772,   284,   307,  1498,   284,  2648,   616,  1998,   351],\n",
            "        [   40,  5465,   428,   523,   881,    11, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256,   464,   717,   640,   314,  2497,   262,\n",
            "           649,   366,   464, 21276,  5542,     1, 12268,    11,   314,   373,\n",
            "           523,  6568,   284,   766,   262,   717, 12268,   329,   262,  7865,\n",
            "          1622,   286,   262,   905,    13,   314,   373,   523,  6568,   284]])\n",
            "Generation complete.\n",
            "\n",
            "Decoding outputs...\n",
            "Decoded texts:\n",
            "Completion to Prompt 1:\n",
            "I've been waiting for a HuggingFace course my whole life, and I'm so glad I did. I'm so happy to be able to share my experience with you.\n",
            "\n",
            "I'm so happy to be able to share my experience with\n",
            "\n",
            "Completion to Prompt 2:\n",
            "I hate this so much,The first time I saw the new \"The Walking Dead\" trailer, I was so excited to see the first trailer for the upcoming season of the show. I was so excited to\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the tokenizer and model\n",
        "checkpoint = \"gpt2\"\n",
        "print(\"Initializing tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "print(\"Initialization complete.\\n\")\n",
        "\n",
        "# Set padding token if not preset\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as padding token\n",
        "    print(\"Padding token set to EOS token.\\n\")\n",
        "\n",
        "# Define sample prompts\n",
        "prompts = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life,\",\n",
        "    \"I hate this so much,\"\n",
        "]\n",
        "print(\"Processing the following prompts:\")\n",
        "for prompt in prompts:\n",
        "    print(f\"- {prompt}\")\n",
        "print()\n",
        "\n",
        "# Tokenize inputs\n",
        "print(\"Tokenizing inputs...\")\n",
        "inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(\"Tokens (input IDs):\", inputs[\"input_ids\"])\n",
        "print(\"Attention masks:\", inputs[\"attention_mask\"])\n",
        "print(\"Tokenization complete.\\n\")\n",
        "\n",
        "# Generate text\n",
        "print(\"Generating text...\")\n",
        "generated_outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "print(\"Tokens (output IDs):\", generated_outputs)\n",
        "print(\"Generation complete.\\n\")\n",
        "\n",
        "# Decode and display the outputs\n",
        "print(\"Decoding outputs...\")\n",
        "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in generated_outputs]\n",
        "print(\"Decoded texts:\")\n",
        "for i, text in enumerate(generated_texts, 1):\n",
        "    print(f\"Completion to Prompt {i}:\\n{text}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}